%%
\documentclass[sigconf]{acmart}
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}


\acmYear{2025}



%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.


%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Word2Vec Comparative Analysis}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Mason Greene}
\email{masonmgreene@gmail.com}
\orcid{1234-5678-9012}
\affiliation{%
  \institution{Olympic College}
  \city{Bremerton}
  \state{Washington}
  \country{USA}
}





%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Mason Greene}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
 Static word embedding models use datasets or "corpora" to determine the relation of the words and assigned their values into a vector representing the semantic meaning of said word to said model. The corpus can be general or specialized to a certain field. Using different models trained on different corpora can highlight the difference a corpora has on the output of a model. This change will be measured by looking at the difference in values of words that occur in both corpora. Nearest Neighbor Score and the Cosine similarity are two ways of checking if the semantic meaning of a given word is the same from model to model. When looking at the result it is clear that the corpora has a significant impact on the model's output. There were some categories of words that generally always had similar neighbors, like places, numbers, and brands. But generally the models had completely different outcomes meaning it's very important to understand the corpora of a model before one can trust the output. 
\end{abstract}


%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}
Word embedding is used everywhere these days due to the booming AI market, that makes understanding the inner workings that much more relevant. The word embedding compared in this analysis is a Static embedding, each word is assigned a vector of values to represent the semantic meaning of the word. The models achieve this by looking at how each word is used in the corpora. This is why understanding the impact or corpora is very important and we can measure this difference of semantic understanding using their Nearest Neighbor Score and their Cosine Similarity. Based off their scores and similarities we can conclude whether or not the different corpora make a difference on the outcome. 


\section{Background}
Word2Vec is a model developed by Google which creates word embeddings represented by numerical vectors. The numbers in the vector represent the semantic meaning of the word compared to other words in the corpus. A corpus is the data the models use to analyze the meaning of words, by looking at how close one word is to another or the frequency the model assigns values in a vector for each word. The semantic meaning is important because by comparing the same word in different models it will result in a semantic shift. This shift is important for understanding the outcomes and biases of the models in question.

\section{Topic Discussion}

This analysis was completed using a comparison program written in Python with NumPy for a linear algebra library and Genism for the model traversal and operations. I used the model GloVe Wiki with 200 dimenssions for my generalized model to compare the specialized models to[1]. For the first specialized model I used SO_vectors_d200 which is trained on corpora from StackOverflow so the specialty is programming text[2]. The second model is Law2Vec_200d which is also 200 dimensions and specializes in law documents[3]. 

As for the means of comparison I used the Nearest Neighbor Score, which is the average if the Jiccard indices of all the shared word from each model. 
A single Jiccard Index
$$
J(w) = \frac{\left|N_k(w, M_1) \cap N_k(w, M_2)\right|}{\left|N_k(w, M_1) \cup N_k(w, M_2)\right|}
$$
The Nearest Neighbor Score equation
$$
NNS_k = \frac{1}{|V|} \sum_{w \in V} J(w)
$$

To be sure these methods are correct they are compared to the Cosine Similarity of each of the words in the model.
Equation for Cosine Similarity
$$
\text{Similarity}(\vec{v}_w^{(1)}, \vec{v}_w^{(2)}) = \frac{\vec{v}_w^{(1)} \cdot \vec{v}_w^{(2)}}{\left\|\vec{v}_w^{(1)}\right\| \left\|\vec{v}_w^{(2)}\right\|} = \frac{\sum_{i=1}^{D} v_{w, i}^{(1)} v_{w, i}^{(2)}}{\sqrt{\sum_{i=1}^{D} (v_{w, i}^{(1)})^2} \sqrt{\sum_{i=1}^{D} (v_{w, i}^{(2)})^2}}
$$

\section{Empirical Results}



\includegraphics[width=0.25\textwidth]{law_nne_2025-12-04.png} %
\includegraphics[width=0.25\textwidth]{so_nne_2025-12-04.png} %
\includegraphics[width=0.5\textwidth]{so_cosine_top_bottom_2025-12-04.png} %
\includegraphics[width=0.5\textwidth]{so_cosine_top_bottom_2025-12-04.png} %
\includegraphics[width=0.25\textwidth]{law_cosine_2025-12-04.png} %
\includegraphics[width=0.25\textwidth]{so_cosine_2025-12-04.png} %
\includegraphics[width=0.5\textwidth]{law_cosine_top_bottom_2025-12-04.png} %
\includegraphics[width=0.5\textwidth]{law_nne_top_bottom_2025-12-04.png} %

The results overwhelmingly prove the corpora has an impact on the semantic meaning of a word. With an average mean of the Nearest Neighbor Score being .05 on a scale of -1 to 1 that is significant. The average mean of the Cosine Similarity for all of the models was -.002. These results clearly state that the corpora is an essential element in determining the output of a model. With that being said there was evidence of categories of words that might be good anchor words, for example, word representation of numbers seem to always be towards the top of the most overlap list. 

\subsection{Configurations}

Languages
\begin{itemize}
    \item Python: 3.11.0
    \item The second bulleted item.
    \item The third bulleted item.
\end{itemize}
IDE
\begin{itemize}
    \item VSCode
\end{itemize}
Libraries
\begin{itemize}
    \item gensim: 4.4.0
    \item numpy: 2.3.5
    \item matplotlib: 3.10.
\end{itemize}
Hardware
\begin{itemize}
    \item 16GB RAM
    \item AMD Ryzen 5 2600
    \item NVIDIA GeForce RTX 3060
\end{itemize}

IDE
VSCode


Libraries
gensim: 4.4.0
numpy: 2.3.5
matplotlib: 3.10.


Hardware
16GB RAM
AMD Ryzen 5 2600
NVIDIA GeForce RTX 3060

\section{Conclusion}
This analysis compared static word embedding models of general corpora to models of specialized corpora. Based on the results of the Nearest Neighbor Score comparison and the comparison of the Cosine Similarity for each word the models shared it is clear that corpora does have an impact on the output of a model. This means that the same word can have two different semantic meanings from one model to another. 




%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-base}
[1] Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global Vectors for Word Representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics.


[2] Vasiliki Efstathiou, Christos Chatzilenas, and Diomidis Spinellis. 2018. Word Embeddings for the Software Engineering Domain. Zenodo. https://doi.org/10.5281/zenodo.1199620

[3] Ilias Chalkidis and Dimitrios Kampas. 2019. Deep learning in law: early adaptation and legal word embeddings trained on large corpora. Artificial Intelligence and Law 27, 2 (June 2019), 171â€“198. https://doi.org/10.1007/s10506-018-9238-9


%%
%% If your work has an appendix, this is the place to put it.
\appendix



\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.
